{"cells":[{"cell_type":"markdown","metadata":{"id":"G0LOaPyAgCU8"},"source":["#  RDDs "]},{"cell_type":"markdown","metadata":{"id":"yxqIrsY-gCVB"},"source":["# Creamos un contexto para crear RDDs"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"T3SkkrkIgCVD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678408368296,"user_tz":-60,"elapsed":48240,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}},"outputId":"9e91d3e7-fbda-4401-b356-fa9b23100a09"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install pyspark --quiet\n","from pyspark import SparkContext"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"_X-TkHs-gCVE","executionInfo":{"status":"ok","timestamp":1678408570343,"user_tz":-60,"elapsed":6386,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}}},"outputs":[],"source":["sc = SparkContext(master = \"local\", appName=\"TransformacionesyAcciones\")"]},{"cell_type":"markdown","metadata":{"id":"oYNeSv5EgCVF"},"source":["# Un RDD es una colección inmutable y distribuida de elementos\n","\n","### Spark automaticamente distribuye los datos y paraleliza las operaciones\n","\n","### Los RDD realmente cargan colecciones de datos"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"-yfNRIqPgCVF","executionInfo":{"status":"ok","timestamp":1678408575956,"user_tz":-60,"elapsed":807,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}}},"outputs":[],"source":["rdd1 = sc.parallelize([1,2,3])"]},{"cell_type":"markdown","metadata":{"id":"7daw3WsugCVF"},"source":["### Debido a la propiedad de distribución que tienen los RDD, en su creación, podemos particionar los datos"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"gV_9CtH-gCVG","executionInfo":{"status":"ok","timestamp":1678408590450,"user_tz":-60,"elapsed":274,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}}},"outputs":[],"source":["import numpy as np\n","rdd2 = sc.parallelize(np.array(range(100)),2)"]},{"cell_type":"markdown","metadata":{"id":"OXf9xyz4gCVG"},"source":["### Verificamos el tipo de dato"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"-6IXNzw2gCVG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678408593250,"user_tz":-60,"elapsed":324,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}},"outputId":"c92b19d2-c3b3-49dd-bf5e-5770fba65903"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["pyspark.rdd.RDD"]},"metadata":{},"execution_count":6}],"source":["type(rdd1)"]},{"cell_type":"markdown","metadata":{"id":"M9fdoJ87gCVG"},"source":["### Vemos el contenido\n","\n","Veremos distintas tecnicas apropiadas para ver el contenido de los RDDs y Dataframes"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"QzqUUjhCgCVH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678408607397,"user_tz":-60,"elapsed":904,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}},"outputId":"40c6e91b-a509-4fa2-9116-b46d47a79107"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[1, 2, 3]"]},"metadata":{},"execution_count":7}],"source":["rdd1.collect()"]},{"cell_type":"markdown","metadata":{"id":"cvXwqRwdgCVH"},"source":["# Carga de un arhivo CSV"]},{"cell_type":"markdown","metadata":{"id":"OfQ67xw1gCVI"},"source":["### Cargamos un RDDs\n","\n","El método textFile busca el archivo en la ruta indicada\n","\n","Cambia el valor de la ruta para que apunte a la ruta donde tienes los datos"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"9AG-1zOcgCVI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1678408634490,"user_tz":-60,"elapsed":18845,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}},"outputId":"c6a6a6dc-f565-43fe-da34-ac0499357e8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')\n","\n","equiposOlimpicosRDD = sc.textFile(\"/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\",2).map(lambda line : line.split(\",\"))"]},{"cell_type":"markdown","metadata":{"id":"FGKXqN4sgCVI"},"source":["### Vemos el contenido\n","\n","El método take es otro método existnte para poder visualizar el contenido de los RDDs"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"HA3o7cq1gCVI","colab":{"base_uri":"https://localhost:8080/","height":832},"executionInfo":{"status":"error","timestamp":1678408640262,"user_tz":-60,"elapsed":322,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}},"outputId":"edd35b9e-0f94-46e1-8f94-0afe1bd70301"},"outputs":[{"output_type":"error","ename":"Py4JJavaError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)","\u001b[0;32m<ipython-input-9-8ff8629e2858>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mequiposOlimpicosRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, num)\u001b[0m\n\u001b[1;32m   1848\u001b[0m         \"\"\"\n\u001b[1;32m   1849\u001b[0m         \u001b[0mitems\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1850\u001b[0;31m         \u001b[0mtotalParts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1851\u001b[0m         \u001b[0mpartsScanned\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/pyspark/rdd.py\u001b[0m in \u001b[0;36mgetNumPartitions\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   3489\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3490\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgetNumPartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3491\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prev_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartitions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3492\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3493\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1322\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1323\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.9/dist-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n","\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o27.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:304)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:292)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:288)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:829)\nCaused by: java.io.IOException: Input path does not exist: file:/content/drive/MyDrive/Colab Notebooks/RED.ES/Datos Ejercicios/M6/paises.csv\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:278)\n\t... 25 more\n"]}],"source":["equiposOlimpicosRDD.take(10)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"LfOUKBkvgCVJ","executionInfo":{"status":"ok","timestamp":1678408648869,"user_tz":-60,"elapsed":1224,"user":{"displayName":"Jabson Palencia","userId":"06337072589842766810"}}},"outputs":[],"source":["sc.stop()"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"provenance":[{"file_id":"1S9cOWohKf8CvPIh2vC8bR68qs2_4qWut","timestamp":1678408144377},{"file_id":"https://github.com/terranigmark/curso-apache-spark-platzi/blob/master/2.%20Primer%20RDD.ipynb","timestamp":1672143190353}]}},"nbformat":4,"nbformat_minor":0}